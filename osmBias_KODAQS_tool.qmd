---
title: "osmBias"
subtitle: "Bias in the Map, Bias in the Results? Validating OpenStreetMap POI Data for
Individual-Level Survey-Based Analysis"
author:
  - name: "Daria Dementeva"
  - name: "Anne-Kathrin Stroppe"
  - name: "author 3"
image: images/tool_icon.png #takes from a folder "images" in your github repository the tool icon file  
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format:
  html: default
  pdf: default
# bibliography: references.bib
biblio-style: apa
---

# At a glance

**How do differences in point of interest (POI) data sources affect analytical results?** \
This tool offers a hands-on, step-by-step workflow to help users compare POI data from OpenStreetMap (OSM) with administrative POI and built environment datasets. The tutorial walks users through the following steps:\

**Step 1a: Prepare the OSM POI data**\
Access OSM POI data, load it into the R  environment, clean and standardize the OSM geometries, and assign POIs to administrative geographic units so they can be compared with official POI data.\

**Step 1b: Create comparable POI indicators**\
Construct the same POI-based measures (e.g., counts) using both OSM and official POI datasets.\

**Step 2: Assess the standalone quality of POI indicators: Coverage**\
Compare POI indicators to identify differences in coverage per administrative geographic unit.\

**Step 3: Link to survey data**\
Link POI-based measures from OSM and official data to survey responses at the respondent's administrative geographic unit of residence.\

**Step 4: Assess impacts on analytical results**\
Estimate parallel regression models using OSM-based and official POI-based indicators. Compare model coefficients, statistical significance, and overall model fit to evaluate how data source differences may influence substantive conclusions.\


### Table of Content

[Introduction](#introduction)

[Setup](#setup)

[Tool application](#tool-application)

[Conclusion and recommendations](#conclusion-and-recommendations)

# Introduction {#introduction}

The growing availability of big geodata enables survey researchers to enrich survey datasets with innovative geocontextual indicators, opening new avenues for research and advancing spatial perspectives in established theoretical frameworks. Against this backdrop, the use of crowdsourced geodata, such as OpenStreetMap, has increased notably in sociological and political science applications. However, linking OSM data to survey responses raises important concerns regarding data quality and fitness for use given the crowdsourced nature of the database.

© OpenStreetMap (licensed under the Open Data Commons Open Database License; https://www.openstreetmap.org/copyright
) is a large-scale, community-driven project providing free, up-to-date geographic data on the built environment, land use, transportation, and administrative boundaries at different spatial and temporal resolutions. As OSM relies on crowdsourced contributions and was not designed for academic or survey research purposes, it has limited formal quality assurance. Nevertheless, in many statistical and research contexts, comparable official data sources are not immediately available, timely, or detailed enough. Therefore, OSM may serve as a viable and cost-efficient alternative, yet systematic data quality assessment of OSM POI data is essential when integrating these data with survey information, as data quality may influence substantive conclusions.

This tool is designed to address this gap. d in a theoretical application that links subjective perceptions of infrastructural deprivation to objective measures of public service provision at the municipal level in Germany. Recent and established political science research shows that residents of under- and low-resourced areas tend to hold more negative views about access to basic infrastructure and services due to their continued exposure to disadvantaged infrastructural conditions. These conditions reflect a community's tangible resources and function as low-intensity informational cues that shape perceptions of accessibility and recognition of adequate service provision (Baybeck & McClurg, 2005; Letki, 2008; McKay, 2019; Stroppe, 2023; Theunissen, 2024). Therefore, one central research question is whether living in an under-resourced area from a public service provision perspective, such as having low availability of schools, supermarkets, and hospitals, is associated with more accentuated perceptions of subjective deprivation. To answer this question, it is fundamental to link survey data with information about respondents' public service provision in their residential environment.

In this application, survey data augmentation and the data linkage with objective measures of public service provision are fundamental to addressing this research question but are prone to certain complicated methodological decisions, such as the choice of the POI data source for proxies of public service provision and its data quality.

For our case study, objective public service provision can be derived from two POI data sources: a German OSM subset and official built environment data from the German Federal Agency for Cartography and Geodesy. Public service provision is measured as the availability of basic services and infrastructure: (1) healthcare (hospitals), (2) education (schools), and (3) food security (supermarkets) in residential areas. POI-based measures from both data sources are aggregated to the municipal level and combined into a group of indicators representing the total number of hospitals, schools, and supermarkets within each municipality, which is then linked to survey respondents based on their municipality of residence. Subjective infrastructural deprivation is measured using individual-level synthetic survey data from the German Longitudinal Election Study capturing respondents' perceptions that society pays insufficient attention to ensuring access to basic infrastructure and services for people like themselves.

This tool addresses these challenges by providing a step-by-step workflow that guides users through the preparation, comparison, and linkage of OSM and official POI data, and by demonstrating how differences in POI data sources can affect empirical results and substantive conclusions in survey-based analyses.

# Data Prerequisites and Description

To run this tool or to apply to their own applications, researchers should need the follwing data:

* Georeferenced survey data that include a geographic reference (e.g., municipality) for each respondent's place of residence, which serves as the data linkage unit for linking POI data and public service provision proxies from both OSM and official POI sources.

* A geodata file of geographic units with geometries (e.g., municipalities) used as the linkage unit for locating POI data across data sources.

* A selected OSM POI data subset.

* Official POI data, if available.

The tool is based on four data sources, in compliance with the above:

* Georeferenced synthetic survey data from the German Longitudinal Election Study 2021, including municipalities of residence

* Shapefiles containing the geometries of German municipalities (2021)

* Shapefiles of points-of-interest (POI) data from the German subset of OpenStreetMap, obtained from Geofabrik (2025)

* Shapefiles of POI data from the Points of Interest Federal Dataset (POI-Bund) of the Federal Agency for Cartography and Geodesy (2022)

# Set-up

## Getting started

### Packages

There are several packages in R to mananage and process geospatial data. We rely on two major packages, such as sp, an established geospatial data processing package, and sp, a more recent package for geospatial data management. The packages dplyr, ggplot2, rlang, and tibble are used for data management, visualization and code support.

```{r}
# Install required packages  

# install.packages(c(
#   "dplyr",
#   "ggplot2",
#   "rlang",
#   "sf",
#   "sp",
#   "tibble"
# ))

```

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Load required packages

library(dplyr) # For data manipulation
library(ggplot2) # For data visualization
library(rlang)  # For code support and code infrastructure management
library(sf) # For handling spatial (geometric) data, established package
library(sp) # For handling spatial (geometric) data, more recent package
library(tibble) # For creating and managing tibbles (special types of data frames)

library(purrr)
library(stringr)
library(tidygeocoder)
```

```{r}
# specify the path in which shapefiles are located 

base <- "C:/Users/daria/Desktop/OSM_data_2025" 

```

### Read in Synthetic Survey and Geospatial Data on German Municipalities

```{r, warning=FALSE}
german_municipalities <- st_read(file.path(base, "municipalites_2021_epsg4326.shp"))

# check coordinate reference system
st_crs(german_municipalities)  

# inspect attributes
head(german_municipalities)  
```

# Tool application {#tool-application}

# Part One: Getting Started with OSM POI Data

### Obtain OSM POI data: A Geofabrik-based Approach

There are numerous ways to obtain OSM data. For example, one can use the OSM Overpass API to obtain custom subsets, the osmdata R package, shapefiles of OSM data extracts from Geofabrik, as well as other web infrastructures such as Protomaps, BBBike, Geoapify, and similar services. All of these data providers capture and render data directly from the global OSM infrastructure, but they differ in the amount of detail, as well as in their geographical and temporal coverage of rendered OSM content.

For complete beginners, as well as for those who are more experienced but prefer working with alternative data formats rather than tabular data, we recommend starting with the shapefiles provided by Geofabrik. Compared to other OSM data providers, Geofabrik offers tidy and de-anonymized comprehensive files (e.g., geometry, place names, coordinates, built-environment feature classes, and other tags) with a selected set of basic yet comprehensive attributes. This data structure and level of content are well suited both for users who are just getting started with OSM and for the analytical purposes of this tool.

How to obtain shapefiles from Geobfabrik? 

**Step 1: Open the Geofabrik download site**/

* Go to: https://download.geofabrik.de/ /

**Step 2: Navigate to your area of interest**/

* On the main page, select the Region (e.g., Europe, Asia, etc.)./

* Select the Sub-Region (e.g., Germany, France, Luxembourg)./

**Step 3: Download the shapefile package**/

* On the sub-region page, locate the shapefile download link named: "XXX-latest-free.shp.zip", where XXX is the sub-region name./

* Download the zip file and save it to your project’s data folder./

**Step 4: Document the download date**/

* Record the date you downloaded the data (DD/MM/YYYY) to ensure methodological transparency (e.g., in a README)./

For Germany, we proceeded as follows: 

**Step 1: Open Geofabrik and select Germany**/

* Go to: https://download.geofabrik.de/

* Select Region = Europe.

* Select Sub-Region = Germany.

**Step 2: Download shapefiles for all federal states**/ 

::: {.callout-note title="FYI"}
Germany is organized by its 16 federal states, so there is no single shapefile download for the whole country.
:::

* On the Germany page, find the list of federal states.

* For each federal state, open its page and download the shapefile ZIP, for example, named hamburg-latest-free.shp.zip.

* Repeat this until you have downloaded shapefiles for all 16 federal states.

**Step 4: Download additional administrative-region shapefiles where applicable**/

::: {.callout-note title="FYI"}
Some federal states are further split into multiple shapefiles corresponding to administrative regions (Regierungsbezirke). 
:::

* Where these subdivisions exist, download all shapefile ZIPs listed for that state (e.g., for Bayern, Mittelfranken, Niederbayern, Oberbayern, Oberfranken, Oberpfalz, Schwaben, Unterfranken, etc.).

**Step 5: Organize and name your files consistently**/

* Create a dedicated folder for the raw downloads

* Save all downloaded ZIP files there.

* Use consistent naming 

**Step 6: Record the download date**/

* The shapefiles were obtained on 11/08/2025.

### Read in OSM POI Data

Once all the shapefiles are obtained, we can read them into R. First, the files are unzipped and their file paths are specified so they can be accessed within the R environment. Because POI in OSM may be represented using different geometries, both point and polygon layers are read to ensure complete coverage of POI features. Point features are defined by longitude and latitude coordinates representing a single location (e.g., a school or hospital), while polygon features represent area extents defined by multiple coordinates forming closed shapes (e.g., hospital complexes or supermarket buildings). Reading both layers ensures that all relevant OSM POI representations are captured.

```{r, results="hide", message=FALSE}

# define objects corresponding to each shapefile

files <- c(  
  arnsberg_regbez_poi = "arnsberg-regbez-latest-free.shp",
  brandenburg_with_berlin_regbez_poi = "brandenburg_with_berlin-latest-free.shp",
  detmold_regbez_poi = "detmold-regbez-latest-free.shp",
  duesseldorf_regbez_poi = "duesseldorf-regbez-latest-free.shp",
  freiburg_regbez_poi = "freiburg-regbez-latest-free.shp",
  hamburg_regbez_poi = "hamburg-latest-free.shp",
  hessen_regbez_poi = "hessen-latest-free.shp",
  karlsruhe_regbez_poi = "karlsruhe-regbez-latest-free.shp",
  koeln_regbez_poi = "koeln-regbez-latest-free.shp",
  mecklenburg_vorpommern_regbez_poi = "mecklenburg-vorpommern-latest-free.shp",
  mittelfranken_regbez_poi = "mittelfranken-latest-free.shp",
  muenster_regbez_poi = "muenster-regbez-latest-free.shp",
  niederbayern_regbez_poi = "niederbayern-latest-free.shp",
  niedersachsen_with_bremen_regbez_poi = "niedersachsen_with_bremen-latest-free.shp",
  oberbayern_regbez_poi = "oberbayern-latest-free.shp",
  oberfranken_regbez_poi = "oberfranken-latest-free.shp",
  oberpfalz_regbez_poi = "oberpfalz-latest-free.shp",
  rheinland_pfalz_regbez_poi = "rheinland-pfalz-latest-free.shp",
  sachsen_anhalt_regbez_poi = "sachsen-anhalt-latest-free.shp",
  saarland_regbez_poi = "saarland-latest-free.shp",
  sachsen_regbez_poi = "sachsen-latest-free.shp",
  schleswig_holstein_regbez_poi = "schleswig-holstein-latest-free.shp",
  schwaben_regbez_poi = "schwaben-latest-free.shp",
  stuttgart_regbez_poi = "stuttgart-regbez-latest-free.shp",
  thueringen_regbez_poi = "thueringen-latest-free.shp",
  tuebingen_regbez_poi = "tuebingen-regbez-latest-free.shp",
  unterfranken_regbez_poi = "unterfranken-latest-free.shp"
)

# read shapefiles for each administrative area, point layer 

for (nm in names(files)) { 
  assign(
    nm,
    st_read(
      dsn = file.path(base, files[[nm]]),
      layer = "gis_osm_pois_free_1", # here: point layer
      quiet = TRUE
    )
  )
}

# read shapefiles for each administrative area, polygon layer 

for (nm in names(files)) {  
  assign(
    paste0(nm, "_poly"),
    st_read(
      dsn = file.path(base, files[[nm]]),
      layer = "gis_osm_pois_a_free_1", # here: polygon layer
      quiet = TRUE
    )
  )
}

# save as .RData as to make it more convenient to work with

save.image("OSM_raw_POI_data.RData")
```

### Extract Public Provision Proxies from OSM 

The workflow below extracts public provision proxies from OSM by filtering, summarizing, and combining both point-based and polygon-based POI datasets. Administrative area-specific identifiers are derived from object names, allowing all area-specific datasets to be stacked into a single nation-wide dataset. The workflow also computes frequency counts of each proxy type by administrative area and geometry type (points and polygons), and finally merges point and polygon features using their shared attributes to produce a unified, nation-wide OSM-based dataset of public provision proxies across Germany.

```{r}
# define OSM feature class that would match public provision proxies 

public_provision_poi <- c("school", "supermarket", "hospital")

# create a helper function to subset the OSM files by the feature class of school, supermarket, hospital

subset_poi <- function(x, keep = public_provision_poi) {
  dplyr::filter(x, fclass %in% keep)
}
```

We create two helper functions to filter each administrative area–specific OSM dataset to retain only schools, supermarkets, and hospitals, derive an area identifier from the object name by removing the suffix, row-bind all regions into a single combined dataset, and produce summary counts by feature class and administrative-area.

```{r}
# helper function to stack all features into one sf table

subset_and_stack <- function(poi_list, suffix_to_remove) {
  imap_dfr(poi_list, ~ subset_poi(.x) %>%
             mutate(region = str_remove(.y, suffix_to_remove)))
}


# helper function to produce summary counts by feature class and administrative-area.
fclass_freq <- function(poi_list, suffix_to_remove) {
  imap_dfr(poi_list, ~ subset_poi(.x) %>%
             st_drop_geometry() %>%
             count(fclass, name = "n") %>%
             mutate(region = str_remove(.y, suffix_to_remove)))
}

```

In this step, we first collect all administrative area-specific OSM point objects whose names end with _regbez_poi into a named list, using pattern matching and mget() to retrieve them from the environment. This list is then passed to helper functions that summarize the number of schools, supermarkets, and hospitals per administrative area and stack all regional datasets into a single nationwide point dataset while retaining region identifiers.

```{r}
# get point objects into a list 
poi_list_points <- mget(ls(pattern = "_regbez_poi$")) |> as.list()

# summarize counts
poi_points_counts <- fclass_freq(poi_list_points, "_regbez_poi$")
print(head(poi_points_counts))

# stack and summarize counts
poi_points_all <- subset_and_stack(poi_list_points, "_regbez_poi$")

# per feature class, nationwide count
table(poi_points_all$fclass)

# overall, all points, nationwide count
nrow(poi_points_all) 
```

Second, the same logic is applied to polygon-based POIs.

```{r}
# get polygon objects into a list
poi_list_polys <- mget(ls(pattern = "_regbez_poi_poly$")) |> as.list()


# summarize counts
poi_polys_counts <- fclass_freq(poi_list_polys, "_regbez_poi_poly$")
print(head(poi_polys_counts))


# stack polygons into one dataset
poi_polys_all <- subset_and_stack(poi_list_polys, "_regbez_poi_poly$")


# per feature class, nation-wide count
table(poi_polys_all$fclass)


# overall, all polygons, nation-wide count
nrow(poi_polys_all)
```

Next, a nationwide OSM-based dataset of public provision proxies across Germany is produced by combining the filtered and stacked point-based and polygon-based POI datasets, retaining only shared attributes to ensure consistency across geometry types.

```{r}
common_cols <- intersect(names(poi_polys_all), names(poi_points_all))

germany_poi_all_combined <- bind_rows(
  poi_polys_all[, common_cols],
  poi_points_all[, common_cols]
)

# per feature class, nation-wide count
table(germany_poi_all_combined$fclass)

# per feature class and per region
region_fclass_counts <- germany_poi_all_combined |>
sf::st_drop_geometry() |>
dplyr::count(region, fclass, name = "n") |>
dplyr::arrange(region, fclass)

print(head(region_fclass_counts))

# overall, nation-wide count
nrow(germany_poi_all_combined) 
```

### Assign POI Data to German Municipalities

Now that we have obtained and extracted public provision proxies across Germany from OSM, we need to assign each school, supermarket, or hospital to a municipality. This will ensure that, later on, we can link these proxies to survey responses using a matching key, namely,  a shared geographical unit across the data sources, such as the municipality, which is assigned to respondents based on their place of residence. 

We ensure that the POI data matches coordinate reference system that of the German municipality polygons. The code then identifies which POI geometries are polygons or multipolygons and replaces those with their centroids (i.e., the most central pair of longitude and latitude) so that all features can be treated as points for spatial overlay with the German municipality polygons. To avoid geometry-related errors during spatial operations, spherical geometry processing is disabled. A spatial join is then performed to attach municipality ID and attributes to each POI based on checking whether each POI point lies within a municipality polygon (i.e., point-in-polygon overlay). Finally, the code checks how many POIs could not be matched to any municipality by counting missing municipality identifiers. 

```{r}

# force sf object
germany_poi_all_combined <- st_as_sf(germany_poi_all_combined)

# ensure CRS is compatiable across data sources
st_crs(germany_poi_all_combined) <- st_crs(german_municipalities)

# identify polygon geometries
geom_type <- st_geometry_type(germany_poi_all_combined)

is_polygon <- geom_type %in% c("POLYGON", "MULTIPOLYGON")

# centroid reduction

# replace polygons with their centroids, keep points as are

germany_poi_all_combined$geometry[is_polygon] <-
  st_centroid(germany_poi_all_combined$geometry[is_polygon])

sf_use_s2(FALSE) # supress errors coming from st_within, force municipality assignment

# overlay OSM public provision proxies with municipalities

poi_with_municipalities <- st_join(
  germany_poi_all_combined,
  german_municipalities,
  join = st_within,
  left = TRUE
)

# Check non-matches

sum(is.na(poi_with_municipalities$mun_id))

```

We identified that 205 were not matched with a municipality. To reveal the reason, we reverse-geocode the unmatched POI. This allows to further diagnose whether the unmatched POI lie outside Germany (e.g., OSM-related error) or artifacts stemming from the point-in-polygon/municipality matching procedure.

```{r}
poi_na <- poi_with_municipalities[is.na(poi_with_municipalities$mun_id), ]

coords <- st_coordinates(poi_na)

poi_na$lon <- coords[, "X"]
poi_na$lat <- coords[, "Y"]

poi_na_rg <- poi_na %>%
  reverse_geocode(
    lat = lat,
    long = lon,
    method = "arcgis",
    address = address, 
    min_time = 1)

poi_na_rg$country_code <- str_extract(str_trim(poi_na_rg$address), "[A-Z]{3}$")

table(poi_na_rg$country_code)
```
This output shows that many POI were not assigned to German municipalities because they are actually located in neighboring countries. Most fall in Czechia (51), Poland (33), Switzerland (47), Austria (32), and France (23), as well as in the Netherlands, Belgium, Luxembourg, and Denmark. This indicates that the municipality unmatches are largely due to POI lying outside Germany (OSM-based error) rather than errors in the spatial join. We can delete the unmatched POI, knowing there are irrelevant and do not belong to Germany administratively.

```{r}
poi_with_municipalities_ger <- poi_with_municipalities[!is.na(poi_with_municipalities$mun_id), ]
```

# Part Two: Obtain official POI data: A POI-Bund-based Approach

# Part Three: Compare Public Service Provision Coverage

# Part Four: Link to Survey Data and Run Models

# Conclusion and recommendations {#conclusion-and-recommendations}

# References


::: {.callout-note title="Custom Note Title"}
This can be additional information you want to provide and visually separate from your main text.
:::

```{=html}
<!-- Additional information can be included as callout blocks with titles, such as
"Be careful", "Good to know", or "Package-specific feature"-->
```